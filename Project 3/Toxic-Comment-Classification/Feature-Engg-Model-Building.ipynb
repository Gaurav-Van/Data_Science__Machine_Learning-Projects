{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# <font color = 'green'>Vectorization</font> and <font color = 'green'>Model Building</font> \n",
    "#### <b>Using TF-IDF and Unigram Approach</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import ComplementNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lazypredict\n",
    "from lazypredict.Supervised import LazyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Function to perform Vectorization and model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vector_model(df, category, vectorizer, ngram):\n",
    "    X = df['comment_text'].fillna(' ')\n",
    "    Y = df[category]\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "    vector = vectorizer(ngram_range=(ngram), stop_words='english')\n",
    "\n",
    "    X_train_scal = vector.fit_transform(X_train)\n",
    "    X_test_scal = vector.transform(X_test)\n",
    "    \n",
    "    #KNN\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train_scal, Y_train)\n",
    "    Y_pred_knn = knn.predict(X_test_scal)\n",
    "    print(f\"Knn done -> It's classification report for {category} category \\n {classification_report(Y_test, Y_pred_knn)} \")\n",
    "    print(\"\\n----------------------------------------------------------------------\")\n",
    "\n",
    "    #logistic regression\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train_scal, Y_train)\n",
    "    Y_pred_lr = lr.predict(X_test_scal)\n",
    "    print(f\"\\nLr done -> It's classification report for {category} category \\n {classification_report(Y_test, Y_pred_lr)} \")\n",
    "    print(\"\\n----------------------------------------------------------------------\\n\")\n",
    "\n",
    "    #Support Vector Machine\n",
    "    svm = SVC(kernel='rbf')\n",
    "    svm.fit(X_train_scal, Y_train)\n",
    "    Y_pred_svm = svm.predict(X_test_scal)\n",
    "    print(f\"\\nsvm done -> It's classification report for {category} category \\n {classification_report(Y_test, Y_pred_svm)} \")\n",
    "    print(\"\\n----------------------------------------------------------------------\\n\")\n",
    "\n",
    "    #Naive Bayes\n",
    "    cnb = ComplementNB()\n",
    "    cnb.fit(X_train_scal, Y_train)\n",
    "    Y_pred_cnb = cnb.predict(X_test_scal)\n",
    "    print(f\"\\ncnb done -> It's classification report for {category} category \\n {classification_report(Y_test, Y_pred_cnb)} \")\n",
    "    print(\"\\n----------------------------------------------------------------------\\n\")\n",
    "\n",
    "    bnb = BernoulliNB()\n",
    "    bnb.fit(X_train_scal, Y_train)\n",
    "    Y_pred_bnb = bnb.predict(X_test_scal)\n",
    "    print(f\"\\nbnb done -> It's classification report for {category} category \\n {classification_report(Y_test, Y_pred_bnb)} \")\n",
    "    print(\"\\n----------------------------------------------------------------------\\n\")\n",
    "\n",
    "    #Decision Tree Classifier\n",
    "    dt = DecisionTreeClassifier(criterion='entropy', min_samples_split=2, random_state=42)\n",
    "    dt.fit(X_train_scal, Y_train)\n",
    "    Y_pred_dt = dt.predict(X_test_scal)\n",
    "    print(f\"\\nDT done -> It's classification report for {category} category \\n {classification_report(Y_test, Y_pred_dt)} \")\n",
    "    print(\"\\n----------------------------------------------------------------------\\n\")\n",
    "\n",
    "    #Random Forest Classifier\n",
    "    rf = RandomForestClassifier(n_estimators=105, min_samples_split=2, random_state=42)\n",
    "    rf.fit(X_train_scal, Y_train)\n",
    "    Y_pred_rf = rf.predict(X_test_scal)\n",
    "    print(f\"\\nRF done -> It's classification report for {category} category \\n {classification_report(Y_test, Y_pred_rf)} \")\n",
    "    print(\"\\n----------------------------------------------------------------------\\n\")\n",
    "\n",
    "    f1_scores = [round(f1_score(Y_pred_knn, Y_test), 2), round(f1_score(Y_pred_lr, Y_test), 2), round(f1_score(Y_pred_svm, Y_test), 2),\n",
    "                 round(f1_score(Y_pred_cnb, Y_test), 2), round(f1_score(Y_pred_bnb, Y_test), 2), round(f1_score(Y_pred_dt, Y_test), 2),\n",
    "                 round(f1_score(Y_pred_rf, Y_test), 2)]\n",
    "    print(f\"F1_scores for {category} category Are calculated\")\n",
    "\n",
    "    Scores = {f'F1_Score - {category}':f1_scores}\n",
    "    Scores_df = pd.DataFrame(Scores, index=['KNN', 'Logistic Regression', 'SVM', 'Complement NB', 'Bernoulli NB', 'Decision Tree', 'Random Forest'])\n",
    "    return Scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knn done -> It's classification report for toxic category \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.63      0.59      4535\n",
      "           1       0.58      0.50      0.54      4642\n",
      "\n",
      "    accuracy                           0.56      9177\n",
      "   macro avg       0.57      0.57      0.56      9177\n",
      "weighted avg       0.57      0.56      0.56      9177\n",
      " \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Lr done -> It's classification report for toxic category \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.89      4535\n",
      "           1       0.92      0.85      0.88      4642\n",
      "\n",
      "    accuracy                           0.88      9177\n",
      "   macro avg       0.89      0.88      0.88      9177\n",
      "weighted avg       0.89      0.88      0.88      9177\n",
      " \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "svm done -> It's classification report for toxic category \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.93      0.89      4535\n",
      "           1       0.92      0.85      0.89      4642\n",
      "\n",
      "    accuracy                           0.89      9177\n",
      "   macro avg       0.89      0.89      0.89      9177\n",
      "weighted avg       0.89      0.89      0.89      9177\n",
      " \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "cnb done -> It's classification report for toxic category \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88      4535\n",
      "           1       0.90      0.86      0.88      4642\n",
      "\n",
      "    accuracy                           0.88      9177\n",
      "   macro avg       0.88      0.88      0.88      9177\n",
      "weighted avg       0.88      0.88      0.88      9177\n",
      " \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "bnb done -> It's classification report for toxic category \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.52      0.65      4535\n",
      "           1       0.66      0.92      0.77      4642\n",
      "\n",
      "    accuracy                           0.72      9177\n",
      "   macro avg       0.76      0.72      0.71      9177\n",
      "weighted avg       0.76      0.72      0.71      9177\n",
      " \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "DT done -> It's classification report for toxic category \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83      4535\n",
      "           1       0.84      0.82      0.83      4642\n",
      "\n",
      "    accuracy                           0.83      9177\n",
      "   macro avg       0.83      0.83      0.83      9177\n",
      "weighted avg       0.83      0.83      0.83      9177\n",
      " \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "RF done -> It's classification report for toxic category \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.92      0.87      4535\n",
      "           1       0.91      0.80      0.85      4642\n",
      "\n",
      "    accuracy                           0.86      9177\n",
      "   macro avg       0.87      0.86      0.86      9177\n",
      "weighted avg       0.87      0.86      0.86      9177\n",
      " \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "F1_scores for toxic category Are calculated\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1_Score - toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Complement NB</th>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernoulli NB</th>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     F1_Score - toxic\n",
       "KNN                              0.54\n",
       "Logistic Regression              0.88\n",
       "SVM                              0.89\n",
       "Complement NB                    0.88\n",
       "Bernoulli NB                     0.77\n",
       "Decision Tree                    0.83\n",
       "Random Forest                    0.85"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Toxic\n",
    "df_toxic = pd.read_csv('Balanced Data/Toxic.csv')\n",
    "result_toxic = vector_model(df_toxic, 'toxic', TfidfVectorizer, (1,1))\n",
    "result_toxic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Severe_toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#severe_toxic\n",
    "df_severe_toxic = pd.read_csv('Balanced Data/Severe_toxic.csv')\n",
    "result_severe_toxic = vector_model(df_severe_toxic, 'severe_toxic', TfidfVectorizer, (1,1))\n",
    "result_severe_toxic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Threat\n",
    "df_threat = pd.read_csv('Balanced Data/Threat.csv')\n",
    "result_threat = vector_model(df_threat, 'threat', TfidfVectorizer, (1,1))\n",
    "result_threat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obscene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Obscene\n",
    "df_obscene = pd.read_csv('Balanced Data/Obscene.csv')\n",
    "result_obscene = vector_model(df_obscene, 'obscene', TfidfVectorizer, (1,1))\n",
    "result_obscene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Insult\n",
    "df_insult = pd.read_csv('Balanced Data/Insult.csv')\n",
    "result_insult = vector_model(df_insult, 'insult', TfidfVectorizer, (1,1))\n",
    "result_insult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identity_hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identity_hate\n",
    "df_identity_hate = pd.read_csv('Balanced Data/Identity_hate.csv')\n",
    "result_identity_hate = vector_model(df_identity_hate, 'identity_hate', TfidfVectorizer, (1,1))\n",
    "result_identity_hate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of F1-Score of all Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualization of F1-Score of all categories\n",
    "result = pd.concat([result_toxic, result_severe_toxic, result_threat, result_obscene, result_insult, result_identity_hate], axis=1)\n",
    "result = result.transpose()\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "sns.lineplot(data=result, markers=True)\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> Trying Test Results - picking logistic regression model from above graph</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Can it Differentiate between <font color = 'red'>Toxic</font> and non-toxic comments</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_toxic.comment_text.fillna(' ')\n",
    "y = df_toxic['toxic']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "tfv1 = TfidfVectorizer(ngram_range=(1,1), stop_words='english')\n",
    "x_train_scal = tfv1.fit_transform(x_train)\n",
    "x_test_scal = tfv1.transform(x_test)\n",
    "lrt = LogisticRegression()\n",
    "lrt.fit(x_train_scal, y_train)\n",
    "lrt.predict(x_test_scal)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Can it Differentiate between <font color = 'red'>Severe Toxic</font> and non-severe-toxic comments</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_severe_toxic.comment_text.fillna(' ')\n",
    "y = df_severe_toxic['severe_toxic']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "tfv2 = TfidfVectorizer(ngram_range=(1,1), stop_words='english')\n",
    "x_train_scal = tfv2.fit_transform(x_train)\n",
    "x_test_scal = tfv2.transform(x_test)\n",
    "lrst = LogisticRegression()\n",
    "lrst.fit(x_train_scal, y_train)\n",
    "lrst.predict(x_test_scal)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Can it Differentiate between <font color = 'red'>Threat</font> and non-threat comments</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_threat.comment_text.fillna(' ')\n",
    "y = df_threat['threat']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "tfv3 = TfidfVectorizer(ngram_range=(1,1), stop_words='english')\n",
    "x_train_scal = tfv3.fit_transform(x_train)\n",
    "x_test_scal = tfv3.transform(x_test)\n",
    "lrth = LogisticRegression()\n",
    "lrth.fit(x_train_scal, y_train)\n",
    "lrth.predict(x_test_scal)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Can it Differentiate between <font color = 'red'>obscene</font> and non-obscene comments</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_obscene.comment_text.fillna(' ')\n",
    "y = df_obscene['obscene']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "tfv4 = TfidfVectorizer(ngram_range=(1,1), stop_words='english')\n",
    "x_train_scal = tfv4.fit_transform(x_train)\n",
    "x_test_scal = tfv4.transform(x_test)\n",
    "lro = LogisticRegression()\n",
    "lro.fit(x_train_scal, y_train)\n",
    "lro.predict(x_test_scal)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Can it Differentiate between <font color = 'red'>Insult</font> and non-Insult comments</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_insult.comment_text.fillna(' ')\n",
    "y = df_insult['insult']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "tfv5 = TfidfVectorizer(ngram_range=(1,1), stop_words='english')\n",
    "x_train_scal = tfv5.fit_transform(x_train)\n",
    "x_test_scal = tfv5.transform(x_test)\n",
    "lri = LogisticRegression()\n",
    "lri.fit(x_train_scal, y_train)\n",
    "lri.predict(x_test_scal)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Can it Differentiate between <font color = 'red'>identity_hate</font> and non-Identity_hate comments</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_identity_hate.comment_text.fillna(' ')\n",
    "y = df_identity_hate['identity_hate']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "tfv6 = TfidfVectorizer(ngram_range=(1,1), stop_words='english')\n",
    "x_train_scal = tfv6.fit_transform(x_train)\n",
    "x_test_scal = tfv6.transform(x_test)\n",
    "lrid = LogisticRegression()\n",
    "lrid.fit(x_train_scal, y_train)\n",
    "lrid.predict(x_test_scal)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = ['7th person on the edge of the cliff is a fucked up person']\n",
    "example2 = ['if you have a look back at the source the information i updated was the correct form i can only guess the source hadnt updated i shall update the information once again but thank you for your message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>toxic or not ?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1_transform = tfv1.transform(example1)\n",
    "zero=lrt.predict_proba(example1_transform)[:,0][0]\n",
    "one=lrt.predict_proba(example1_transform)[:,1][0]\n",
    "if (zero>=0.42 and one<=0.58) and (zero<=0.58 and one>=0.42):\n",
    "    print('Neutral for Toxic Category')\n",
    "elif one>0.58:\n",
    "    print('Toxic')\n",
    "else: \n",
    "    print('Non Toxic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example2_transform = tfv1.transform(example2)\n",
    "zero=lrt.predict_proba(example2_transform)[:,0][0]\n",
    "one=lrt.predict_proba(example2_transform)[:,1][0]\n",
    "if (zero>=0.42 and one<=0.58) and (zero<=0.58 and one>=0.42):\n",
    "    print('Neutral for Toxic Category')\n",
    "elif one>0.58:\n",
    "    print('Toxic')\n",
    "else: \n",
    "    print('Non Toxic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>severe_toxic or not ?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1_transform = tfv2.transform(example1)\n",
    "zero=lrst.predict_proba(example1_transform)[:,0][0]\n",
    "one=lrst.predict_proba(example1_transform)[:,1][0]\n",
    "if (zero>=0.42 and one<=0.58) and (zero<=0.58 and one>=0.42):\n",
    "    print('Neutral for Severe Toxic Category')\n",
    "elif one>0.58:\n",
    "    print('Severe Toxic')\n",
    "else: \n",
    "    print('Non Severe Toxic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example2_transform = tfv2.transform(example2)\n",
    "zero=lrst.predict_proba(example2_transform)[:,0][0]\n",
    "one=lrst.predict_proba(example2_transform)[:,1][0]\n",
    "if (zero>=0.42 and one<=0.58) and (zero<=0.58 and one>=0.42):\n",
    "    print('Neutral for Severe Toxic Category')\n",
    "elif one>0.58:\n",
    "    print('Severe Toxic')\n",
    "else: \n",
    "    print('Non Severe Toxic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>threat or not ?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1_transform = tfv3.transform(example1)\n",
    "zero=lrth.predict_proba(example1_transform)[:,0][0]\n",
    "one=lrth.predict_proba(example1_transform)[:,1][0]\n",
    "if (zero>=0.42 and one<=0.58) and (zero<=0.58 and one>=0.42):\n",
    "    print('Neutral for Threat Category')\n",
    "elif one>0.58:\n",
    "    print('Threat')\n",
    "else: \n",
    "    print('Non Threat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example2_transform = tfv3.transform(example2)\n",
    "zero=lrth.predict_proba(example2_transform)[:,0][0]\n",
    "one=lrth.predict_proba(example2_transform)[:,1][0]\n",
    "if (zero>=0.42 and one<=0.58) and (zero<=0.58 and one>=0.42):\n",
    "    print('Neutral for Threat Category')\n",
    "elif one>0.58:\n",
    "    print('Threat')\n",
    "else: \n",
    "    print('Non Threat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>obscene or not ?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1_transform = tfv4.transform(example1)\n",
    "zero=lro.predict_proba(example1_transform)[:,0][0]\n",
    "one=lro.predict_proba(example1_transform)[:,1][0]\n",
    "if (zero>=0.42 and one<=0.58) and (zero<=0.58 and one>=0.42):\n",
    "    print('Neutral for Obscene Category')\n",
    "elif one>0.58:\n",
    "    print('Obscene')\n",
    "else: \n",
    "    print('Non Obscene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example2_transform = tfv4.transform(example2)\n",
    "zero=lro.predict_proba(example2_transform)[:,0][0]\n",
    "one=lro.predict_proba(example2_transform)[:,1][0]\n",
    "if (zero>=0.42 and one<=0.58) and (zero<=0.58 and one>=0.42):\n",
    "    print('Neutral for Obscene Category')\n",
    "elif one>0.58:\n",
    "    print('Obscene')\n",
    "else: \n",
    "    print('Non Obscene')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>insult or not ?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1_transform = tfv5.transform(example1)\n",
    "zero=lri.predict_proba(example1_transform)[:,0][0]\n",
    "one=lri.predict_proba(example1_transform)[:,1][0]\n",
    "if (zero>=0.42 and one<=0.58) and (zero<=0.58 and one>=0.42):\n",
    "    print('Neutral for Insult Category')\n",
    "elif one>0.58:\n",
    "    print('Insult')\n",
    "else: \n",
    "    print('Non Insult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example2_transform = tfv5.transform(example2)\n",
    "zero=lri.predict_proba(example2_transform)[:,0][0]\n",
    "one=lri.predict_proba(example2_transform)[:,1][0]\n",
    "if (zero>=0.42 and one<=0.58) and (zero<=0.58 and one>=0.42):\n",
    "    print('Neutral for Insult Category')\n",
    "elif one>0.58:\n",
    "    print('Insult')\n",
    "else: \n",
    "    print('Non Insult')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>identity_hate or not ?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1_transform = tfv6.transform(example1)\n",
    "zero=lrid.predict_proba(example1_transform)[:,0][0]\n",
    "one=lrid.predict_proba(example1_transform)[:,1][0]\n",
    "if (zero>=0.42 and one<=0.58) and (zero<=0.58 and one>=0.42):\n",
    "    print('Neutral for Identity Hate Category')\n",
    "elif one>0.58:\n",
    "    print('Identity hate')\n",
    "else: \n",
    "    print('Non Identity hate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example2_transform = tfv6.transform(example2)\n",
    "zero=lrid.predict_proba(example2_transform)[:,0][0]\n",
    "one=lrid.predict_proba(example2_transform)[:,1][0]\n",
    "if (zero>=0.42 and one<=0.58) and (zero<=0.58 and one>=0.42):\n",
    "    print('Neutral for Identity Hate Category')\n",
    "elif one>0.58:\n",
    "    print('Identity hate')\n",
    "else: \n",
    "    print('Non Identity hate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Trained Models as Pickle Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfiles(df, label):\n",
    "    x = df.comment_text.fillna(' ')\n",
    "    y = df[label]\n",
    "    \n",
    "    tfv_f = TfidfVectorizer(ngram_range=(1,1), stop_words='english')\n",
    "    X_vect = tfv_f.fit_transform(x)\n",
    "    \n",
    "    with open(f'{label + \"_vect\"}.pkl', 'wb') as f:\n",
    "        pickle.dump(tfv_f, f)\n",
    "    \n",
    "    log = LogisticRegression()\n",
    "    log.fit(X_vect, y)\n",
    "    \n",
    "    with open(f'{label + \"_model\"}.pkl', 'wb') as f:\n",
    "        pickle.dump(log, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_c = ['toxic', 'severe_toxic', 'threat', 'obscene', 'insult', 'identity_hate']\n",
    "list_d = [df_toxic, df_severe_toxic, df_threat, df_obscene, df_insult, df_identity_hate]\n",
    "for i, j in zip(list_d, list_c):\n",
    "    getfiles(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
